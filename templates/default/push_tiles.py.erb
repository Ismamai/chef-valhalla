#!/usr/bin/env python
from __future__ import print_function
import sys
import os
import math
import time
import boto
from filechunkio import FileChunkIO

#do some maintainance on s3, including pushing the new data up there
def push_to_s3(file_name, bucket_name = '<%= node[:valhalla][:bucket] %>', bucket_dir = '<%= node[:valhalla][:bucket_dir] %>'):
  #check what we have in s3
  s3 = boto.connect_s3()
  bucket = s3.get_bucket(bucket_name)
  keys = sorted([ k.key for k in bucket.get_all_keys() if k.key.startswith(bucket_dir + '/tiles_') if k.key.endswith('.tgz') ])
  keys.reverse()

  #lets keep it to a managable amount of previous ones
  while len(keys) > 50:
    key = keys.pop()
    bucket.delete_key(key)

  #get ready for multi part
  source_size = os.stat(file_name).st_size
  key_name = bucket_dir + '/' + os.path.basename(file_name)
  multipart = bucket.initiate_multipart_upload(key_name)
  chunk_size = 536870912
  chunk_count = int(math.ceil(float(source_size) / float(chunk_size)))

  #send them
  print('Pushing %s to s3' % file_name)
  for i in range(chunk_count):
    offset = chunk_size * i
    bytes = min(chunk_size, source_size - offset)
    with FileChunkIO(file_name, 'r', offset=offset, bytes=bytes) as fp:
      multipart.upload_part_from_file(fp, part_num=i + 1)
      print('Part pushed to s3 %%%f' % (100 * float(offset + bytes) / float(source_size)))
  multipart.complete_upload()

  #check its ok and make it public
  key = bucket.get_key(key_name)
  if key is None:
    raise Exception('Failed to push file to s3')
  key.make_public()

#get all the instances of a layer and run some recipes on each
def update_instances(stack = '<%= node[:valhalla][:service_stack] %>', layer = '<%= node[:valhalla][:service_layer] %>', recipes = list('<%= node[:valhalla][:service_recipes] %>'.split(','))):
  #connect to opsworks
  opsworks = boto.connect_opsworks()

  #grab all the instances for that layer
  instances = opsworks.describe_instances(layer_id=layer)

  #TODO: make this more robust of a heuristic
  #TODO: sort by instance name so that we always try the same one first
  #if we dont have enough machines to feel safe we give up
  instances = [ instance for instance in instances['Instances'] if instance.get('Status') == 'online' ]
  if len(instances) < 3:
    raise Exception('Not enough instances in layer to risk the update')

  #for each one
  for instance in instances:
    #deploy task or something..
    deployment_id = opsworks.create_deployment(stack, {'Name': 'execute_recipes', 'Args': {'recipes': recipes}}, instance_ids=[instance['InstanceId']])['DeploymentId']
    #wait until it ends
    while True:
      if opsworks.describe_commands(deployment_id=deployment_id)['Status'] != 'pending':
        break
      time.sleep(5)
    #check that its status is all good
    if opsworks.describe_commands(deployment_id=deployment_id)['Status'] != 'successful':
      print('Deployment %s to Instance %s failed' % (deployment_id, instance['InstanceId']), file=sys.stderr) 
      sys.exit(1)

#entry point for script
if __name__ == "__main__":
  if len(sys.argv) < 2:
    print('Wrong arguments', file=sys.stderr)
    sys.exit(1)

  #push them to s3
  push_to_s3(sys.argv[1], '<%= node[:valhalla][:bucket] %>', '<%= node[:valhalla][:bucket_dir] %>')

  #update the service instances
  update_instances('<%= node[:valhalla][:service_stack] %>', '<%= node[:valhalla][:service_layer] %>', list('<%= node[:valhalla][:service_recipes] %>'.split(',')))
